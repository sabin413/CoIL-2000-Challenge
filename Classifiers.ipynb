{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regession, Neural Network, and XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a13e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from platform import python_version\n",
    "#python_version()\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0b720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the feature description txt. file and make a list of feature names\n",
    "features_list = [] \n",
    "fh = open('TicDataDescr.txt', 'r')\n",
    "for i, line in enumerate(fh):\n",
    "    if 32 < i < 51: # 204:\n",
    "        features_list.append(line.rstrip('\\n')[2:])\n",
    "    elif 51 <= i< 204:\n",
    "        features_list.append(line.rstrip('\\n')[3:])\n",
    "fh.close()\n",
    "\n",
    "while '' in features_list:\n",
    "    features_list.remove('')\n",
    "#print(features_list)\n",
    "#len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba839a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_table('ticdata2000.txt', names=features_list) # assign the feature names from the list prepared before\n",
    "#df_test = pd.read_table('ticeval2000.txt', names=features_list) # test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ebd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 5.977327378907592\n"
     ]
    }
   ],
   "source": [
    "# Find the total number of positive class in the target - baseline\n",
    "ls = list(df['CARAVAN Number of mobile home policies 0 - 1'].values)\n",
    "num_policy_users = ls.count(1)\n",
    "num_nonusers = ls.count(0)\n",
    "users_perc = num_policy_users*100/(num_policy_users+num_nonusers)\n",
    "print(num_policy_users, users_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940f918",
   "metadata": {},
   "source": [
    "We have a baseline for our models here, 348 out of 5822 customers, which is about 6%, own the insurence policy. This means only 6% of the total customers belong to the positive class and hence the data is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0065c9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE Customer Subtype see L0</th>\n",
       "      <th>MAANTHUI Number of houses 1 – 10</th>\n",
       "      <th>MGEMOMV Avg size household 1 – 6</th>\n",
       "      <th>MGEMLEEF Avg age see L1</th>\n",
       "      <th>MOSHOOFD Customer main type see L2</th>\n",
       "      <th>MGODRK Roman catholic see L3</th>\n",
       "      <th>MGODPR Protestant ...</th>\n",
       "      <th>MGODOV Other religion</th>\n",
       "      <th>MGODGE No religion</th>\n",
       "      <th>MRELGE Married</th>\n",
       "      <th>...</th>\n",
       "      <th>APERSONG Number of private accident insurance policies</th>\n",
       "      <th>AGEZONG Number of family accidents insurance policies</th>\n",
       "      <th>AWAOREG Number of disability insurance policies</th>\n",
       "      <th>ABRAND Number of fire policies</th>\n",
       "      <th>AZEILPL Number of surfboard policies</th>\n",
       "      <th>APLEZIER Number of boat policies</th>\n",
       "      <th>AFIETS Number of bicycle policies</th>\n",
       "      <th>AINBOED Number of property insurance policies</th>\n",
       "      <th>ABYSTAND Number of social security insurance policies</th>\n",
       "      <th>CARAVAN Number of mobile home policies 0 - 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.253349</td>\n",
       "      <td>1.110615</td>\n",
       "      <td>2.678805</td>\n",
       "      <td>2.991240</td>\n",
       "      <td>5.773617</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>4.626932</td>\n",
       "      <td>1.069907</td>\n",
       "      <td>3.258502</td>\n",
       "      <td>6.183442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.570079</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.059773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.846706</td>\n",
       "      <td>0.405842</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.814589</td>\n",
       "      <td>2.856760</td>\n",
       "      <td>1.003234</td>\n",
       "      <td>1.715843</td>\n",
       "      <td>1.017503</td>\n",
       "      <td>1.597647</td>\n",
       "      <td>1.909482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.080532</td>\n",
       "      <td>0.077403</td>\n",
       "      <td>0.562058</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>0.210986</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.119996</td>\n",
       "      <td>0.237087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MOSTYPE Customer Subtype see L0  MAANTHUI Number of houses 1 – 10   \n",
       "count                      5822.000000                       5822.000000  \\\n",
       "mean                         24.253349                          1.110615   \n",
       "std                          12.846706                          0.405842   \n",
       "min                           1.000000                          1.000000   \n",
       "25%                          10.000000                          1.000000   \n",
       "50%                          30.000000                          1.000000   \n",
       "75%                          35.000000                          1.000000   \n",
       "max                          41.000000                         10.000000   \n",
       "\n",
       "       MGEMOMV Avg size household 1 – 6  MGEMLEEF Avg age see L1   \n",
       "count                       5822.000000              5822.000000  \\\n",
       "mean                           2.678805                 2.991240   \n",
       "std                            0.789835                 0.814589   \n",
       "min                            1.000000                 1.000000   \n",
       "25%                            2.000000                 2.000000   \n",
       "50%                            3.000000                 3.000000   \n",
       "75%                            3.000000                 3.000000   \n",
       "max                            5.000000                 6.000000   \n",
       "\n",
       "       MOSHOOFD Customer main type see L2  MGODRK Roman catholic see L3   \n",
       "count                         5822.000000                   5822.000000  \\\n",
       "mean                             5.773617                      0.696496   \n",
       "std                              2.856760                      1.003234   \n",
       "min                              1.000000                      0.000000   \n",
       "25%                              3.000000                      0.000000   \n",
       "50%                              7.000000                      0.000000   \n",
       "75%                              8.000000                      1.000000   \n",
       "max                             10.000000                      9.000000   \n",
       "\n",
       "       MGODPR Protestant ...  MGODOV Other religion  MGODGE No religion   \n",
       "count            5822.000000            5822.000000         5822.000000  \\\n",
       "mean                4.626932               1.069907            3.258502   \n",
       "std                 1.715843               1.017503            1.597647   \n",
       "min                 0.000000               0.000000            0.000000   \n",
       "25%                 4.000000               0.000000            2.000000   \n",
       "50%                 5.000000               1.000000            3.000000   \n",
       "75%                 6.000000               2.000000            4.000000   \n",
       "max                 9.000000               5.000000            9.000000   \n",
       "\n",
       "       MRELGE Married  ...   \n",
       "count     5822.000000  ...  \\\n",
       "mean         6.183442  ...   \n",
       "std          1.909482  ...   \n",
       "min          0.000000  ...   \n",
       "25%          5.000000  ...   \n",
       "50%          6.000000  ...   \n",
       "75%          7.000000  ...   \n",
       "max          9.000000  ...   \n",
       "\n",
       "       APERSONG Number of private accident insurance policies   \n",
       "count                                        5822.000000       \\\n",
       "mean                                            0.005325        \n",
       "std                                             0.072782        \n",
       "min                                             0.000000        \n",
       "25%                                             0.000000        \n",
       "50%                                             0.000000        \n",
       "75%                                             0.000000        \n",
       "max                                             1.000000        \n",
       "\n",
       "       AGEZONG Number of family accidents insurance policies   \n",
       "count                                        5822.000000      \\\n",
       "mean                                            0.006527       \n",
       "std                                             0.080532       \n",
       "min                                             0.000000       \n",
       "25%                                             0.000000       \n",
       "50%                                             0.000000       \n",
       "75%                                             0.000000       \n",
       "max                                             1.000000       \n",
       "\n",
       "       AWAOREG Number of disability insurance policies   \n",
       "count                                      5822.000000  \\\n",
       "mean                                          0.004638   \n",
       "std                                           0.077403   \n",
       "min                                           0.000000   \n",
       "25%                                           0.000000   \n",
       "50%                                           0.000000   \n",
       "75%                                           0.000000   \n",
       "max                                           2.000000   \n",
       "\n",
       "       ABRAND Number of fire policies  AZEILPL Number of surfboard policies   \n",
       "count                     5822.000000                           5822.000000  \\\n",
       "mean                         0.570079                              0.000515   \n",
       "std                          0.562058                              0.022696   \n",
       "min                          0.000000                              0.000000   \n",
       "25%                          0.000000                              0.000000   \n",
       "50%                          1.000000                              0.000000   \n",
       "75%                          1.000000                              0.000000   \n",
       "max                          7.000000                              1.000000   \n",
       "\n",
       "       APLEZIER Number of boat policies  AFIETS Number of bicycle policies   \n",
       "count                       5822.000000                        5822.000000  \\\n",
       "mean                           0.006012                           0.031776   \n",
       "std                            0.081632                           0.210986   \n",
       "min                            0.000000                           0.000000   \n",
       "25%                            0.000000                           0.000000   \n",
       "50%                            0.000000                           0.000000   \n",
       "75%                            0.000000                           0.000000   \n",
       "max                            2.000000                           3.000000   \n",
       "\n",
       "       AINBOED Number of property insurance policies   \n",
       "count                                    5822.000000  \\\n",
       "mean                                        0.007901   \n",
       "std                                         0.090463   \n",
       "min                                         0.000000   \n",
       "25%                                         0.000000   \n",
       "50%                                         0.000000   \n",
       "75%                                         0.000000   \n",
       "max                                         2.000000   \n",
       "\n",
       "       ABYSTAND Number of social security insurance policies   \n",
       "count                                        5822.000000      \\\n",
       "mean                                            0.014256       \n",
       "std                                             0.119996       \n",
       "min                                             0.000000       \n",
       "25%                                             0.000000       \n",
       "50%                                             0.000000       \n",
       "75%                                             0.000000       \n",
       "max                                             2.000000       \n",
       "\n",
       "       CARAVAN Number of mobile home policies 0 - 1  \n",
       "count                                   5822.000000  \n",
       "mean                                       0.059773  \n",
       "std                                        0.237087  \n",
       "min                                        0.000000  \n",
       "25%                                        0.000000  \n",
       "50%                                        0.000000  \n",
       "75%                                        0.000000  \n",
       "max                                        1.000000  \n",
       "\n",
       "[8 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f97b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d5456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.680906</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>0.217444</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>-0.161816</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992297</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>0.302552</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>-0.096077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992297</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>0.914172</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>-1.667319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.187437</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>-0.970980</td>\n",
       "      <td>1.299414</td>\n",
       "      <td>-0.948264</td>\n",
       "      <td>0.914172</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>-0.619824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.225840</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>1.672893</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>1.479559</td>\n",
       "      <td>0.302552</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>0.914449</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>-2.125697</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>0.800298</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>-0.787790</td>\n",
       "      <td>-2.714813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>0.836602</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>1.672893</td>\n",
       "      <td>1.238473</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>0.302552</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>-0.096077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>0.680906</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>1.238473</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>0.800298</td>\n",
       "      <td>-1.051594</td>\n",
       "      <td>-0.161816</td>\n",
       "      <td>-0.619824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>0.764971</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>0.758754</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>-1.216964</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>1.383152</td>\n",
       "      <td>-1.051594</td>\n",
       "      <td>-0.787790</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>-1.014358</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>0.680906</td>\n",
       "      <td>-0.27258</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.779405</td>\n",
       "      <td>-0.694311</td>\n",
       "      <td>0.800298</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>-0.787790</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>-0.05992</td>\n",
       "      <td>-1.014358</td>\n",
       "      <td>-0.022706</td>\n",
       "      <td>-0.07365</td>\n",
       "      <td>-0.15062</td>\n",
       "      <td>-0.087348</td>\n",
       "      <td>-0.118816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5822 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1         2         3         4         5         6   \n",
       "0     0.680906 -0.27258  0.406697 -1.216964  0.779405 -0.694311  0.217444  \\\n",
       "1     0.992297 -0.27258 -0.859500 -1.216964  0.779405  0.302552 -0.365410   \n",
       "2     0.992297 -0.27258 -0.859500 -1.216964  0.779405 -0.694311 -0.365410   \n",
       "3    -1.187437 -0.27258  0.406697  0.010755 -0.970980  1.299414 -0.948264   \n",
       "4     1.225840 -0.27258  1.672893 -1.216964  1.479559  0.302552 -0.365410   \n",
       "...        ...      ...       ...       ...       ...       ...       ...   \n",
       "5817  0.914449 -0.27258 -2.125697 -1.216964  0.779405 -0.694311  0.800298   \n",
       "5818  0.836602 -0.27258  1.672893  1.238473  0.779405  0.302552 -0.365410   \n",
       "5819  0.680906 -0.27258  0.406697  1.238473  0.779405 -0.694311  0.800298   \n",
       "5820  0.758754 -0.27258  0.406697 -1.216964  0.779405 -0.694311  1.383152   \n",
       "5821  0.680906 -0.27258  0.406697  0.010755  0.779405 -0.694311  0.800298   \n",
       "\n",
       "             7         8         9  ...        76        77       78   \n",
       "0    -0.068711 -0.161816  0.427670  ... -0.073165 -0.081055 -0.05992  \\\n",
       "1    -0.068711  0.464159 -0.096077  ... -0.073165 -0.081055 -0.05992   \n",
       "2     0.914172  0.464159 -1.667319  ... -0.073165 -0.081055 -0.05992   \n",
       "3     0.914172  0.464159 -0.619824  ... -0.073165 -0.081055 -0.05992   \n",
       "4    -0.068711  0.464159  0.427670  ... -0.073165 -0.081055 -0.05992   \n",
       "...        ...       ...       ...  ...       ...       ...      ...   \n",
       "5817 -0.068711 -0.787790 -2.714813  ... -0.073165 -0.081055 -0.05992   \n",
       "5818 -0.068711  0.464159 -0.096077  ... -0.073165 -0.081055 -0.05992   \n",
       "5819 -1.051594 -0.161816 -0.619824  ... -0.073165 -0.081055 -0.05992   \n",
       "5820 -1.051594 -0.787790  0.427670  ... -0.073165 -0.081055 -0.05992   \n",
       "5821 -0.068711 -0.787790  0.427670  ... -0.073165 -0.081055 -0.05992   \n",
       "\n",
       "            79        80       81       82        83        84  target  \n",
       "0     0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "1     0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "2     0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "3     0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "4     0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "...        ...       ...      ...      ...       ...       ...     ...  \n",
       "5817  0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "5818  0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "5819  0.764971 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       1  \n",
       "5820 -1.014358 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "5821 -1.014358 -0.022706 -0.07365 -0.15062 -0.087348 -0.118816       0  \n",
       "\n",
       "[5822 rows x 86 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('CARAVAN Number of mobile home policies 0 - 1',axis = 1) #feature\n",
    "y = df['CARAVAN Number of mobile home policies 0 - 1'] #target\n",
    "\n",
    "#rescale X\n",
    "x = X.values #nd array\n",
    "stan_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = stan_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df['target'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b8543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with Upsampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40326001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARAVAN Number of mobile home policies 0 - 1\n",
      "0    4113\n",
      "1     253\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CARAVAN Number of mobile home policies 0 - 1\n",
       "0    4113\n",
       "1    4113\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.25, random_state=0)\n",
    "print(y_train.value_counts()) # count the numbers of 0 and 1 before sampling\n",
    "smt = SMOTE(random_state = 0)\n",
    "X_train_os, y_train_os = smt.fit_resample(X_train, y_train)\n",
    "y_train_os.value_counts()\n",
    "#print(np.bincount(y_train1)) # count the numbers of 0 and 1 after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44aed84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs',max_iter=1000) # instantiate the model\n",
    "logreg.fit(X_train_os,y_train_os) # fit the data\n",
    "y_pred = logreg.predict(X_test) \n",
    "#X['MOSTYPE Customer Subtype see L0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e362c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[981, 380],\n",
       "       [ 42,  53]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation by confusion matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a249dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 23.52222222222222, 'Predicted label')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the confusion matrix by using a heatmap\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11504aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7101648351648352\n",
      "Precision: 0.12240184757505773\n",
      "Recall: 0.5578947368421052\n"
     ]
    }
   ],
   "source": [
    "# model evaluation by accuracy, precission and Recall\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5233f",
   "metadata": {},
   "source": [
    "Out of all predicted as positive, 12 percentage are actually positive (precision), which is significantly better than the 6 perc baseline.\n",
    "Out of all positive cases from the data, 56 percentages are correctly predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "318bddaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3019, 1094],\n",
       "       [  66,  187]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = logreg.predict(X_train)\n",
    "cnf_matrix = metrics.confusion_matrix(y_train, y_pred_train)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d37751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7343105817682088\n",
      "Precision: 0.14597970335675253\n",
      "Recall: 0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred_train))\n",
    "print(\"Precision:\",metrics.precision_score(y_train, y_pred_train))\n",
    "print(\"Recall:\",metrics.recall_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5648e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tune hyperparameter - Logistic regression doesn't really nead hyperparameter tuining, we do it anayway\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=10, random_state=0, max_iter = 100, 1000)\n",
    "clf.fit(X_train_os,y_train_os) # fit the data\n",
    "y_pred = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a93ec43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[980, 381],\n",
       "       [ 42,  53]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c79832",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (tune the algorethim used, and the strength of regularization) marginally improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b04c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Recursive Feature Ellimination - Since we saw many \n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77911327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False  True False  True False False  True  True  True\n",
      "  True  True  True False False False False False  True False False False\n",
      "  True False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False  True False False  True  True\n",
      " False  True  True False  True False  True False False  True  True False\n",
      " False False False False  True False  True False  True False False  True\n",
      " False False False  True False False  True  True False  True False False\n",
      " False]\n",
      "[ 1  1 40 13  1 37  1 41 36  1  1  1  1  1  1 43 32  7  5 25  1  4 39  6\n",
      "  1 27 28  3 29  1  1  1  1  1  1  1  1  1  1  1 31 30 24  1 18 10  1  1\n",
      " 14  1  1 45  1 16  1 38 33  1  1 23 11 34  8 42  1 46  1 35  1 44 15  1\n",
      " 26 17  2  1 12 20  1  1 22  1 19  9 21]\n"
     ]
    }
   ],
   "source": [
    "#logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, n_features_to_select=40, step=1)\n",
    "rfe = rfe.fit(X_train_os, y_train_os)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0aff496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8226,)\n",
      "(1456,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_os[:,0].shape)\n",
    "print(X_test[:,0].shape)\n",
    "support_list = list(rfe.support_)\n",
    "X_train_rfe = np.empty(shape = (8226, 40))\n",
    "X_test_rfe = np.empty(shape = (1456, 40))\n",
    "#X_test_rfe  = \n",
    "#print(len(support_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4627b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for ind, elem in enumerate(support_list):\n",
    "    if elem == True:\n",
    "        X_train_rfe[:,j] = X_train_os[:,ind]\n",
    "        X_test_rfe[:,j] = X_test[:,ind]\n",
    "        j += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43069249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 40)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rfe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "263bb098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_rfe, y_train_os) # fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "629e8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_rfe) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe116283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[962, 399],\n",
       "       [ 42,  53]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d5433d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6971153846153846\n",
      "Precision: 0.1172566371681416\n",
      "Recall: 0.5578947368421052\n"
     ]
    }
   ],
   "source": [
    "# model evaluation by accuracy, precission and Recall\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a257f9a",
   "metadata": {},
   "source": [
    "Performes slightly worse, I tried different values (from 4 to 40) for the no.s of features, none of them improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1873f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c7ebb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbbc52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86f79011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architeture\n",
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(80, activation = 'relu'),\n",
    "        Dense(40, activation = 'relu'),\n",
    "        Dense(1, activation = 'sigmoid'), \n",
    "    ])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ed3a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ") #0.001 is actually the default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d571004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 3ms/step - loss: 0.5201\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3740\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.2929\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.2341\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.1905\n",
      "Epoch 6/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.1543\n",
      "Epoch 7/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.1324\n",
      "Epoch 8/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.1117\n",
      "Epoch 9/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0983\n",
      "Epoch 10/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0858\n",
      "Epoch 11/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0787\n",
      "Epoch 12/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0688\n",
      "Epoch 13/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0682\n",
      "Epoch 14/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0606\n",
      "Epoch 15/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0559\n",
      "Epoch 16/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0520\n",
      "Epoch 17/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0457\n",
      "Epoch 18/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0455\n",
      "Epoch 19/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0443\n",
      "Epoch 20/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0478\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "history = model.fit(X_train_os, y_train_os, epochs=20, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "193a2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_baseline(loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \\\n",
    "#                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)):\n",
    "    \n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(40, activation='relu', input_dim=n_cols))\n",
    "#    model.add(Dense(20, activation='relu'))\n",
    "#    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "#    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy']) \n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8448c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n",
      "[[1258  103]\n",
      " [  75   20]]\n",
      "Accuracy: 0.8777472527472527\n",
      "Precision: 0.16260162601626016\n",
      "Recall: 0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b4c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 2ms/step\n",
      "[[4027   86]\n",
      " [   5  248]]\n",
      "Accuracy: 0.9791571232249199\n",
      "Precision: 0.7425149700598802\n",
      "Recall: 0.9802371541501976\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
    "cm = metrics.confusion_matrix(y_train, y_pred)\n",
    "#print(y_pred)\n",
    "## calculate model performance\n",
    "cm = metrics.confusion_matrix(y_train, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_train, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad769af8",
   "metadata": {},
   "source": [
    " It shows overfitting, lets plot the loss (J) (for both train and test) as a function of time. We expect a higher loss for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86246fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5201407670974731,\n",
       " 0.3739963173866272,\n",
       " 0.29294097423553467,\n",
       " 0.23412832617759705,\n",
       " 0.19051694869995117,\n",
       " 0.15428388118743896,\n",
       " 0.1324107050895691,\n",
       " 0.11168496310710907,\n",
       " 0.09834987670183182,\n",
       " 0.08580894768238068,\n",
       " 0.07873447239398956,\n",
       " 0.06877138465642929,\n",
       " 0.0682087317109108,\n",
       " 0.06064306199550629,\n",
       " 0.05592813715338707,\n",
       " 0.052024755626916885,\n",
       " 0.04567667841911316,\n",
       " 0.045518822968006134,\n",
       " 0.0443032942712307,\n",
       " 0.04784354567527771]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_loss\n",
    "#val_acc = history.history['val_acc']\n",
    "#loss = history.history['loss']\n",
    "#val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b293e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1835aa76820>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epo = range(20)\n",
    "plt.plot(epo, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec265208",
   "metadata": {},
   "outputs": [],
   "source": [
    "## grid search CV\n",
    "epochs = [10, 20, 30]\n",
    "rate = [0.0005, 0.001, 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a757a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba3abb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(80, activation = 'relu'))\n",
    "    model.add(Dense(40, activation = 'relu'))\n",
    "    model.add(Dense(20, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0bc95ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbnad\\AppData\\Local\\Temp\\ipykernel_7788\\1426700400.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  base_model = KerasClassifier(build_fn=create_baseline, verbose = 0)\n"
     ]
    }
   ],
   "source": [
    "base_model = KerasClassifier(build_fn=create_baseline, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e54e7053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 4ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "52/52 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "parameters = {'batch_size': [32, 64, 128],\n",
    "              'epochs': [10, 20, 30],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = base_model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "grid_search = grid_search.fit(X_train_os, y_train_os, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de1c70f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'epochs': 30, 'optimizer': 'rmsprop'}\n",
      "0.9643845815775187\n"
     ]
    }
   ],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(best_parameters)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a359482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "129/129 [==============================] - 1s 3ms/step - loss: 0.0517\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0394\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0406\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0412\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0371\n",
      "Epoch 6/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0364\n",
      "Epoch 7/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0345\n",
      "Epoch 8/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0376\n",
      "Epoch 9/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0360\n",
      "Epoch 10/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "Epoch 11/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0359\n",
      "Epoch 12/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0342\n",
      "Epoch 13/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0317\n",
      "Epoch 14/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0332\n",
      "Epoch 15/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0316\n",
      "Epoch 16/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0380\n",
      "Epoch 17/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0288\n",
      "Epoch 18/20\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0315\n",
      "Epoch 19/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0282\n",
      "Epoch 20/20\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0293\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    ")\n",
    "\n",
    "history = model.fit(X_train_os, y_train_os, epochs=20, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc20be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n",
      "[[1270   91]\n",
      " [  78   17]]\n",
      "Accuracy: 0.8839285714285714\n",
      "Precision: 0.1574074074074074\n",
      "Recall: 0.17894736842105263\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "#print(y_pred)\n",
    "## calculate model performance\n",
    "print(cm)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4660f",
   "metadata": {},
   "source": [
    "So the best NN has 89 perc accuracy, 16 perc precision, and recall of 15 perc. A few observations: The training results are much better \n",
    "than the test results suggesting overfitting. I tried to mitigate it by decreasing the size of the neural network,\n",
    "but it did not really work. When can change the network arctiture and the hyperparameters to bring the training \n",
    "efficiency lower, the test results again get extremely bad. In other words, I find a huge performance gap between \n",
    "training and testing. Ideally, I would try to reduce the gap by not overfitting the neural network, but in this case,\n",
    "I do not seem to suceed. Is there a resolution, or I should just accept that this is the limitation of this data and model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff09d54",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5400ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13e21e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other classifiers\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05861603",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I will do GCV for XGB\n",
    "df = pd.DataFrame()\n",
    "n_estimators = [20,50]\n",
    "max_depth = [4,6,8]\n",
    "eta = [0.01, 0.1]\n",
    "param = [[i,j,k] for i in n_estimators for j in max_depth for k in eta]\n",
    "acc, pre, rec = [], [], []\n",
    "acct, pret, rect = [], [], []\n",
    "for para in param:\n",
    "    model = XGBClassifier(n_estimators=para[0], max_depth=para[1], eta=para[2], subsample=0.7, colsample_bytree=0.8)\n",
    "    model.fit(X_train_os, y_train_os)\n",
    "    y_pred = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
    "    #cm = metrics.confusion_matrix(y_train, y_pred)\n",
    "    #print(model)\n",
    "    #print(cm)\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    #print(\"Precision:\",metrics.precision_score(y_train, y_pred))\n",
    "    #print(\"Recall:\",metrics.recall_score(y_train, y_pred))\n",
    "    acc.append(metrics.accuracy_score(y_train, y_pred))\n",
    "    pre.append(metrics.precision_score(y_train, y_pred))\n",
    "    rec.append(metrics.recall_score(y_train, y_pred))\n",
    "    \n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    #cm = metrics.confusion_matrix(y_train, y_pred)\n",
    "    #print(model)\n",
    "    #print(cm)\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))\n",
    "    #print(\"Precision:\",metrics.precision_score(y_train, y_pred))\n",
    "    #print(\"Recall:\",metrics.recall_score(y_train, y_pred))\n",
    "    acct.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    pret.append(metrics.precision_score(y_test, y_pred))\n",
    "    rect.append(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "df['params'] = param\n",
    "df['train_accuracy'] = acc\n",
    "df['train_precision'] = pre\n",
    "df['train_recall'] = rec\n",
    "\n",
    "df['test_accuracy'] = acct\n",
    "df['test_precision'] = pret\n",
    "df['test_recall'] = rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b290158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[20, 4, 0.01]</td>\n",
       "      <td>0.827073</td>\n",
       "      <td>0.157104</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.800824</td>\n",
       "      <td>0.137546</td>\n",
       "      <td>0.389474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[20, 4, 0.1]</td>\n",
       "      <td>0.876088</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.442688</td>\n",
       "      <td>0.857830</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>0.326316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20, 6, 0.01]</td>\n",
       "      <td>0.880440</td>\n",
       "      <td>0.215645</td>\n",
       "      <td>0.403162</td>\n",
       "      <td>0.861264</td>\n",
       "      <td>0.183432</td>\n",
       "      <td>0.326316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20, 6, 0.1]</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.356890</td>\n",
       "      <td>0.399209</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.221053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[20, 8, 0.01]</td>\n",
       "      <td>0.914567</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.411067</td>\n",
       "      <td>0.885989</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.242105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[20, 8, 0.1]</td>\n",
       "      <td>0.939762</td>\n",
       "      <td>0.480159</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.147368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[50, 4, 0.01]</td>\n",
       "      <td>0.828676</td>\n",
       "      <td>0.165088</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.809753</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[50, 4, 0.1]</td>\n",
       "      <td>0.934952</td>\n",
       "      <td>0.408284</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.914835</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.136842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[50, 6, 0.01]</td>\n",
       "      <td>0.883646</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.403162</td>\n",
       "      <td>0.866071</td>\n",
       "      <td>0.191358</td>\n",
       "      <td>0.326316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[50, 6, 0.1]</td>\n",
       "      <td>0.948465</td>\n",
       "      <td>0.607692</td>\n",
       "      <td>0.312253</td>\n",
       "      <td>0.923764</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.073684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[50, 8, 0.01]</td>\n",
       "      <td>0.916628</td>\n",
       "      <td>0.328173</td>\n",
       "      <td>0.418972</td>\n",
       "      <td>0.883929</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[50, 8, 0.1]</td>\n",
       "      <td>0.960834</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.063158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           params  train_accuracy  train_precision  train_recall   \n",
       "0   [20, 4, 0.01]        0.827073         0.157104      0.454545  \\\n",
       "1    [20, 4, 0.1]        0.876088         0.218750      0.442688   \n",
       "2   [20, 6, 0.01]        0.880440         0.215645      0.403162   \n",
       "3    [20, 6, 0.1]        0.923500         0.356890      0.399209   \n",
       "4   [20, 8, 0.01]        0.914567         0.317073      0.411067   \n",
       "5    [20, 8, 0.1]        0.939762         0.480159      0.478261   \n",
       "6   [50, 4, 0.01]        0.828676         0.165088      0.482213   \n",
       "7    [50, 4, 0.1]        0.934952         0.408284      0.272727   \n",
       "8   [50, 6, 0.01]        0.883646         0.222222      0.403162   \n",
       "9    [50, 6, 0.1]        0.948465         0.607692      0.312253   \n",
       "10  [50, 8, 0.01]        0.916628         0.328173      0.418972   \n",
       "11   [50, 8, 0.1]        0.960834         0.797101      0.434783   \n",
       "\n",
       "    test_accuracy  test_precision  test_recall  \n",
       "0        0.800824        0.137546     0.389474  \n",
       "1        0.857830        0.178161     0.326316  \n",
       "2        0.861264        0.183432     0.326316  \n",
       "3        0.898352        0.221053     0.221053  \n",
       "4        0.885989        0.196581     0.242105  \n",
       "5        0.901786        0.184211     0.147368  \n",
       "6        0.809753        0.147287     0.400000  \n",
       "7        0.914835        0.236364     0.136842  \n",
       "8        0.866071        0.191358     0.326316  \n",
       "9        0.923764        0.233333     0.073684  \n",
       "10       0.883929        0.175439     0.210526  \n",
       "11       0.920330        0.181818     0.063158  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce5d41",
   "metadata": {},
   "source": [
    "As can be seen from the table, within the range of the parameter space explored the model performance \n",
    "doesn't improve. So, the conclusion is that Logistic regression seems to be performing the best. We tried logistic regression classifier, Neural network, and XG Boost and logistic regression worked the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
